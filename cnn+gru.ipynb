{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import skimage\n",
    "from skimage.transform import resize\n",
    "from scipy.misc import imread\n",
    "from collections import Counter\n",
    "from time import time\n",
    "\n",
    "from utils import load_dataset, load_vocab, get_batch\n",
    "\n",
    "def load_image(path):\n",
    "    # load image\n",
    "    img = imread(path,mode='RGB')\n",
    "    img = img / 255.0\n",
    "    assert (0 <= img).all() and (img <= 1.0).all()\n",
    "    #print \"Original Image Shape: \", img.shape\n",
    "    # we crop image from center\n",
    "    short_edge = min(img.shape[:2])\n",
    "    yy = int((img.shape[0] - short_edge) / 2)\n",
    "    xx = int((img.shape[1] - short_edge) / 2)\n",
    "    crop_img = img[yy : yy + short_edge, xx : xx + short_edge]\n",
    "    # resize to 224, 224\n",
    "    resized_img = resize(crop_img, (224, 224))\n",
    "    return resized_img\n",
    "\n",
    "def extract_features(ids, path, output_path, extractor, batch_size=64):\n",
    "    images_names = dict()\n",
    "    for p in listdir(path):\n",
    "        image_id = int(p.split('_')[-1].split('.')[0])\n",
    "        if image_id in ids:\n",
    "            images_names[image_id] = p\n",
    "    batch,names = [],[]\n",
    "    with open(output_path,'w') as output_file:\n",
    "        for idx,n in enumerate(images_names):\n",
    "            p = join(path, images_names[n])\n",
    "            batch.append(load_image(p))\n",
    "            names.append(n)\n",
    "            if len(batch)==batch_size:\n",
    "                batch = np.stack(batch)\n",
    "                feed_dict = {images: batch}\n",
    "                with tf.device('/gpu:0'):\n",
    "                    features = sess.run(extractor, feed_dict=feed_dict)\n",
    "                for n,f in zip(names,features):\n",
    "                    output_file.write(\"%s;%s\\n\" % (n, \" \".join(str(x) for x in f)))\n",
    "                print(\"%d/%d\" % (idx,len(images_names)))\n",
    "                batch, names = [],[]\n",
    "                output_file.flush()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all the COCOQA images are in train/val MSCOCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: /home/hbenyounes/vqa/datasets/train2014\n",
      "Train : 46293\n",
      "Train absent from source : 0\n",
      "Source: /home/hbenyounes/vqa/datasets/val2014\n",
      "Test : 22879\n",
      "Test absent from source : 0\n"
     ]
    }
   ],
   "source": [
    "train_ids = set([int(j) for j in open('datasets/coco/train/img_ids.txt','r')])\n",
    "\n",
    "source = '/home/hbenyounes/vqa/datasets/train2014'\n",
    "image_ids = set()\n",
    "for p in listdir(source):\n",
    "    image_id = int(p.split('_')[-1].split('.')[0])\n",
    "    image_ids.add(image_id)\n",
    "\n",
    "print(\"Source: %s\" % source)\n",
    "print(\"Train : %d\\nTrain absent from source : %d\" % (len(train_ids), len(train_ids-image_ids)))\n",
    "\n",
    "\n",
    "test_ids = set([int(j) for j in open('datasets/coco/test/img_ids.txt','r')])\n",
    "source = '/home/hbenyounes/vqa/datasets/val2014'\n",
    "image_ids = set()\n",
    "for p in listdir(source):\n",
    "    image_id = int(p.split('_')[-1].split('.')[0])\n",
    "    image_ids.add(image_id)\n",
    "\n",
    "print(\"Source: %s\" % source)    \n",
    "print(\"Test : %d\\nTest absent from source : %d\" % (len(test_ids), len(test_ids-image_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all the visual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"tensorflow-vgg16/vgg16.tfmodel\", mode='rb') as f:\n",
    "    fileContent = f.read()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(fileContent)\n",
    "\n",
    "    images = tf.placeholder(\"float\", [None, 224, 224, 3])\n",
    "\n",
    "    tf.import_graph_def(graph_def, input_map={ \"images\": images })\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    out_tensor = graph.get_tensor_by_name(\"import/Relu_1:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/46293\n",
      "129/46293\n",
      "194/46293\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-d9149fdd80e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m extract_features(train_ids,'/home/hbenyounes/vqa/datasets/train2014', \n\u001b[1;32m---> 27\u001b[1;33m                  \"/home/hbenyounes/vqa/datasets/coco/train/images.feat\", out_tensor)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-d9149fdd80e1>\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(ids, path, output_path, extractor, batch_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                 \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-40bb23c05652>\u001b[0m in \u001b[0;36mload_image\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mcrop_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0myy\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0myy\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mshort_edge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mxx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mshort_edge\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# resize to 224, 224\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mresized_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrop_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresized_img\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3/dist-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(image, output_shape, order, mode, cval)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         out = warp(image, tform, output_shape=output_shape, order=order,\n\u001b[1;32m---> 90\u001b[1;33m                    mode=mode, cval=cval)\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3/dist-packages/skimage/transform/_geometric.py\u001b[0m in \u001b[0;36mwarp\u001b[1;34m(image, inverse_map, map_args, output_shape, order, mode, cval, reverse_map)\u001b[0m\n\u001b[0;32m   1059\u001b[0m                 dims.append(_warp_fast(image[..., dim], matrix,\n\u001b[0;32m   1060\u001b[0m                             \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m                             order=order, mode=mode, cval=cval))\n\u001b[0m\u001b[0;32m   1062\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0morig_ndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_warps_cy.pyx\u001b[0m in \u001b[0;36mskimage.transform._warps_cy._warp_fast (skimage/transform/_warps_cy.c:2127)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mascontiguousarray\u001b[1;34m(a, dtype)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m     \"\"\"\n\u001b[1;32m--> 569\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masfortranarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_features(ids, path, output_path, extractor, batch_size=64):\n",
    "    images_names = dict()\n",
    "    for p in listdir(path):\n",
    "        image_id = int(p.split('_')[-1].split('.')[0])\n",
    "        if image_id in ids:\n",
    "            images_names[image_id] = p\n",
    "    batch,names = [],[]\n",
    "    with open(output_path,'w') as output_file:\n",
    "        for idx,n in enumerate(images_names):\n",
    "            p = join(path, images_names[n])\n",
    "            if len(batch)<batch_size:\n",
    "                batch.append(load_image(p))\n",
    "                names.append(n)\n",
    "            else:\n",
    "                batch = np.stack(batch)\n",
    "                feed_dict = {images: batch}\n",
    "                with tf.device('/gpu:0'):\n",
    "                    features = sess.run(extractor, feed_dict=feed_dict)\n",
    "                for n,f in zip(names,features):\n",
    "                    output_file.write(\"%s;%s\\n\" % (n, \" \".join(str(x) for x in f)))\n",
    "                print(\"%d/%d\" % (idx,len(images_names)))\n",
    "                batch, names = [],[]\n",
    "                output_file.flush()\n",
    "                \n",
    "                \n",
    "extract_features(train_ids,'/home/hbenyounes/vqa/datasets/train2014', \n",
    "                 \"/home/hbenyounes/vqa/datasets/coco/train/images.feat\", out_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/46293\n",
      "129/46293\n",
      "194/46293\n",
      "259/46293\n",
      "324/46293\n",
      "389/46293\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'JpegImageFile' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-0c4cf854d414>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"IMSHAPE = 2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-40bb23c05652>\u001b[0m in \u001b[0;36mload_image\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# load image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m#print \"Original Image Shape: \", img.shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'JpegImageFile' and 'float'"
     ]
    }
   ],
   "source": [
    "ids = train_ids \n",
    "path = '/home/hbenyounes/vqa/datasets/train2014'\n",
    "output_path = \"/home/hbenyounes/vqa/datasets/coco/train/images.feat\" \n",
    "extractor = out_tensor\n",
    "batch_size = 64\n",
    "images_names = dict()\n",
    "for p in listdir(path):\n",
    "    image_id = int(p.split('_')[-1].split('.')[0])\n",
    "    if image_id in ids:\n",
    "        images_names[image_id] = p\n",
    "batch,names = [],[]\n",
    "with open(output_path,'w') as output_file:\n",
    "    for idx,n in enumerate(images_names):\n",
    "        p = join(path, images_names[n])\n",
    "        if len(batch)<batch_size:\n",
    "            im = load_image(p)\n",
    "            if len(im.shape) == 2:\n",
    "                print(\"IMSHAPE = 2\")\n",
    "                break\n",
    "            batch.append(im)\n",
    "            names.append(n)\n",
    "        else:\n",
    "            batch = np.stack(batch)\n",
    "            feed_dict = {images: batch}\n",
    "            with tf.device('/gpu:0'):\n",
    "                features = sess.run(extractor, feed_dict=feed_dict)\n",
    "            for n,f in zip(names,features):\n",
    "                output_file.write(\"%s;%s\\n\" % (n, \" \".join(str(x) for x in f)))\n",
    "            print(\"%d/%d\" % (idx,len(images_names)))\n",
    "            batch, names = [],[]\n",
    "            output_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import skimage\n",
    "from skimage.transform import resize\n",
    "from scipy.misc import imread\n",
    "from collections import Counter\n",
    "from time import time\n",
    "\n",
    "from utils import load_dataset, load_vocab, get_batch\n",
    "\n",
    "def read_features(path,n_max = np.Inf):\n",
    "    feats = {}\n",
    "    for n,l in enumerate(open(path,encoding='latin1')):\n",
    "        l = l.strip().split(\";\")\n",
    "        idx = l[0]\n",
    "        feat = [float(x) for x in l[1].split()]\n",
    "        feats[idx] = feat\n",
    "        if n>n_max:\n",
    "            break\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse features file\n",
      "Parse questions file\n",
      "Parse answers file\n"
     ]
    }
   ],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, f_path, i_path, q_path, a_path):\n",
    "        self.f_path = f_path\n",
    "        self.i_path = i_path\n",
    "        self.q_path = q_path\n",
    "        self.a_path = a_path\n",
    "        print('Parse features file')\n",
    "        f_data = read_features(self.f_path,n_max=50)\n",
    "        print('Parse questions file')\n",
    "        q_data = load_dataset(self.q_path)\n",
    "        self.max_q = len(max(q_data, key=lambda x:len(x)))\n",
    "        print('Parse answers file')\n",
    "        a_data = load_dataset(self.a_path)\n",
    "        self.data = []\n",
    "        for q_id,q,a in zip(open(i_path),q_data,a_data):\n",
    "            q_id = q_id.strip()\n",
    "            try:\n",
    "                f = f_data[q_id]\n",
    "            except:\n",
    "                continue\n",
    "            datum = (f_data[q_id],q,a)\n",
    "            self.data.append(datum)\n",
    "        del f_data,q_data,a_data\n",
    "        self.N = len(self.data)\n",
    "        self.indexes = np.arange(self.N)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def batch_gen(self,batch_size=64):\n",
    "        batch = []\n",
    "        np.random.shuffle(self.indexes)\n",
    "        for idx in self.indexes:\n",
    "            batch.append(self.data[self.indexes[idx]])\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if len(batch)>0:\n",
    "            yield batch\n",
    "        \n",
    "\n",
    "f_path = \"/home/hbenyounes/vqa/datasets/coco/test/images.feat\"\n",
    "i_path = \"/home/hbenyounes/vqa/datasets/coco/test/img_ids.txt\"\n",
    "q_path = \"/home/hbenyounes/vqa/datasets/coco/test/questions.idxs\"\n",
    "a_path = \"/home/hbenyounes/vqa/datasets/coco/test/answers.idxs\"\n",
    "\n",
    "dataset = Dataset(f_path, i_path, q_path, a_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_feed_dict(batch,max_q,Na):\n",
    "    batch_size = len(batch)\n",
    "    V = np.zeros((batch_size, len(batch[0][0])), 'float32')\n",
    "    Q = np.zeros((batch_size, max_q), 'int32')\n",
    "    mask = np.zeros((max_q,batch_size), 'int32')\n",
    "    ans = np.zeros((batch_size,Na),'int32')\n",
    "    \n",
    "    for i,(im,s,a) in enumerate(batch):\n",
    "        V[i] = im\n",
    "        Q[i] = np.pad(s, (0,max_q-len(s)), 'constant')\n",
    "        mask[len(s)-1,i] = 1\n",
    "        ans[i,a] = 1\n",
    "    mask = mask[:,:,None]\n",
    "    return V,Q,mask,ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_gen = dataset.batch_gen(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_i2w, q_w2i = load_vocab('datasets/coco/train/questions.vocab')\n",
    "a_i2w, a_w2i = load_vocab('datasets/coco/train/answers.vocab')\n",
    "Nq = len(q_i2w)\n",
    "Na = len(a_i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot copy sequence with size 4096 to array axis with dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a2e97dc1d2a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mb_gen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_feed_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_q\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-0fd82afa5f14>\u001b[0m in \u001b[0;36mcreate_feed_dict\u001b[1;34m(batch, max_q, Na)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mV\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_q\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'constant'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot copy sequence with size 4096 to array axis with dimension 3"
     ]
    }
   ],
   "source": [
    "for batch in b_gen:\n",
    "    V,Q,mask,ans = create_feed_dict(batch,dataset.max_q,Na)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

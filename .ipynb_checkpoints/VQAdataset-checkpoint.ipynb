{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Create the json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want an easy-to-read json file for the dataset containing the following keys:\n",
    "\n",
    "* image_id : the id of the image (type:int)\n",
    "* question : sentence (type:str)\n",
    "* ground_truth : a list of true answers (type:[str])\n",
    "* multiple_choice : a list of possible answers (type:[str])\n",
    "\n",
    "Note that both questions and answers are to be expressed in the SAME id2word table ! We can then have the freedom to tie/untie the word embedding matrices in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "val\n"
     ]
    }
   ],
   "source": [
    "split = 'train'\n",
    "for split in 'train val'.split():\n",
    "    print(split)\n",
    "    D = json.loads(open('datasets/vqa/%s/MultipleChoice_mscoco_%s2014_questions.json' %(split,split)).read())\n",
    "    A = json.loads(open('datasets/vqa/%s/mscoco_%s2014_annotations.json' %(split,split)).read())\n",
    "    dataset = open('datasets/vqa/%s/dataset.json'%split,'w')\n",
    "    for a,d in zip(A['annotations'],D['questions']):\n",
    "        datapoint = {}\n",
    "        datapoint['image_id'] = a['image_id']\n",
    "        datapoint['ground_truth'] = [x['answer'] for x in a['answers']]\n",
    "        datapoint['multiple_choices'] = d['multiple_choices']\n",
    "        datapoint['question'] = d['question']\n",
    "        dataset.write(json.dumps(datapoint)+'\\n')\n",
    "    dataset.close()\n",
    "    del A,D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Create the vocabulary\n",
    "\n",
    "We are going to take all the words appearing in the training set (question, ground truth answers and multiple choices), lowercase them and build a vocabulary out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/248350\n",
      "12417/248350\n",
      "24834/248350\n",
      "37251/248350\n",
      "49668/248350\n",
      "62085/248350\n",
      "74502/248350\n",
      "86919/248350\n",
      "99336/248350\n",
      "111753/248350\n",
      "124170/248350\n",
      "136587/248350\n",
      "149004/248350\n",
      "161421/248350\n",
      "173838/248350\n",
      "186255/248350\n",
      "198672/248350\n",
      "211089/248350\n",
      "223506/248350\n",
      "235923/248350\n",
      "248340/248350\n",
      "Couldn't write â€™\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "vocabulary = Counter()\n",
    "N = len(open('datasets/vqa/train/dataset.json').read().split('\\n'))\n",
    "for i,l in enumerate(open('datasets/vqa/train/dataset.json')):\n",
    "    if not i % (N//20):\n",
    "        print(\"%d/%d\" % (i,N))\n",
    "    l = json.loads(l)\n",
    "    sent = wordpunct_tokenize(l['question'].lower())\n",
    "    for ans in l['multiple_choices'] + l['ground_truth']:\n",
    "        sent += wordpunct_tokenize(ans)\n",
    "    vocabulary.update(Counter(sent))\n",
    "vocab_file = open('datasets/vqa/vocabulary.txt','w')\n",
    "vocab_file.write('<unk>\\n<s>\\n</s>\\n')\n",
    "for w,c in vocabulary.most_common():\n",
    "    try:\n",
    "        vocab_file.write(w + '\\n')\n",
    "    except:\n",
    "        print(\"Couldn't write %s\" % w)\n",
    "vocab_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Integerify the text\n",
    "\n",
    "Here, we will take the vocabulary created in #2 and the json dataset from #1 to make a new dataset_idxs.json where all the words are replaced by their indexes in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "val\n"
     ]
    }
   ],
   "source": [
    "def integerify(sent,vocab):\n",
    "    tokenized = wordpunct_tokenize(sent)\n",
    "    output = []\n",
    "    for w in tokenized:\n",
    "        if w in vocab:\n",
    "            output.append(vocab[w])\n",
    "        else:\n",
    "            output.append(vocab['<unk>'])\n",
    "    return output\n",
    "w2i = {}\n",
    "for i,l in enumerate(open('datasets/vqa/vocabulary.txt','r')):\n",
    "    w2i[l.strip()] = i\n",
    "\n",
    "for split in 'train val'.split():\n",
    "    print(split)\n",
    "    dataset_idxs = open('datasets/vqa/%s/dataset_idxs.json'%split,'w')\n",
    "    dataset = open('datasets/vqa/%s/dataset.json' % split,'r')\n",
    "    for l in dataset:\n",
    "        l = json.loads(l)\n",
    "        l['question'] = integerify(l['question'], w2i)\n",
    "        for key in \"multiple_choices ground_truth\".split():\n",
    "            X = []\n",
    "            for x in l[key]:\n",
    "                integ = integerify(x,w2i)\n",
    "                if len(integ)==0:\n",
    "                    print(\"PROBLEM WITH LINE %d\" % i)\n",
    "                X.append(integ)\n",
    "            l[key] = X\n",
    "        dataset_idxs.write(json.dumps(l)+'\\n')\n",
    "\n",
    "    dataset.close()\n",
    "    dataset_idxs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_triplets(D,A):\n",
    "    questions, image_ids, answers, mcs =[],[],[],[]\n",
    "    for d,a in zip(D['questions'],A['annotations']):\n",
    "        assert d['image_id'] == a['image_id']\n",
    "        questions.append(d['question'])\n",
    "        image_ids.append(d['image_id'])\n",
    "        answers.append(a['multiple_choice_answer'])\n",
    "        mcs.append(' | '.join(d['multiple_choices']))\n",
    "    print(\"%d questions (%d different)\" % (len(questions), len(set(questions))))\n",
    "    print(\"%d images (%d different)\" % (len(image_ids), len(set(image_ids))))\n",
    "    return questions, image_ids, answers, mcs\n",
    "\n",
    "def extract_spatial_triplets(questions,image_ids,answers,mcs):\n",
    "    words = open('spatial_words.txt','r').read().split()\n",
    "\n",
    "    IQA = set()\n",
    "    for q,i,a,mc in zip(questions, image_ids,answers,mcs):\n",
    "        qwords = set(q.lower().strip('?').split())\n",
    "        if len(qwords.intersection(words))>=1:\n",
    "            IQA.add((i,q,a,mc))\n",
    "    spatial_questions = []\n",
    "    spatial_image_ids = []\n",
    "    spatial_answers = []\n",
    "    spatial_mcs = []\n",
    "    for i,q,a,mc in IQA:\n",
    "        spatial_questions.append(q)\n",
    "        spatial_image_ids.append(i)\n",
    "        spatial_answers.append(a)\n",
    "        spatial_mcs.append(mc)\n",
    "    print(\"%d questions (%d different)\" % (len(spatial_questions), len(set(spatial_questions))))\n",
    "    print(\"%d images (%d different)\" % (len(spatial_image_ids), len(set(spatial_image_ids))))\n",
    "    return spatial_questions, spatial_image_ids, spatial_answers, spatial_mcs\n",
    "\n",
    "def write_triplets(spatial_questions,spatial_image_ids,spatial_answers,spatial_mcs,split):\n",
    "    image_file = open('datasets/vqa/'+split+'/img_ids.txt','w',encoding='utf-8')\n",
    "    question_file = open('datasets/vqa/'+split+'/questions.txt','w',encoding='utf-8')\n",
    "    answer_file = open('datasets/vqa/'+split+'/answers.txt','w',encoding='utf-8')\n",
    "    mc_file = open('datasets/vqa/'+split+'/mcs.txt','w',encoding='utf-8')\n",
    "    for i,q,a,mc in zip(spatial_image_ids, spatial_questions, spatial_answers,spatial_mcs):\n",
    "        image_file.write(str(i) + '\\n')\n",
    "        question_file.write(q.lower().strip('?') + '\\n')\n",
    "        mc_file.write(mc+'\\n')\n",
    "        answer_file.write(a +'\\n')\n",
    "    image_file.close()\n",
    "    question_file.close()\n",
    "    answer_file.close()\n",
    "    mc_file.close()\n",
    "    return 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248349 questions (152050 different)\n",
      "248349 images (82783 different)\n",
      "121512 questions (81565 different)\n",
      "121512 images (40504 different)\n"
     ]
    }
   ],
   "source": [
    "for split in 'train val'.split():\n",
    "    D = json.loads(open('datasets/vqa/'+split+'/MultipleChoice_mscoco_'+split+'2014_questions.json').read())\n",
    "    A = json.loads(open('datasets/vqa/'+split+'/mscoco_'+split+'2014_annotations.json').read())\n",
    "    O = json.loads(open('datasets/vqa/'+split+'/OpenEnded_mscoco_'+split+'2014_questions.json').read())\n",
    "    questions,image_ids,answers,mcs = extract_triplets(D,A)\n",
    "    write_triplets(questions,image_ids,answers,mcs,split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
